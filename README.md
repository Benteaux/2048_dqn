# 2048_dqn

A homemade DQN to play the game 2048.

The program uses browser automation via Selenium (sometimes referred to as the "driver," "webdriver" or "game") to open up an instance of the 2048 game and send signals (up, down, left, right) to the server.
The driver can also read in the state of the game (score & tile locations/numbers), detect if the game has ended and start a new game. Occasionally the message "Error occurred" may pop up - this is to indicate that there was a delay
in sending / processing the new state of the game, and the driver is trying to send/process the new state again.

The actual DQN is created from scratch with PyTorch. The DQN file has a variety of functions for saving metrics, loading metrics, displaying metrics, saving model weights (both the policy & target network, as well as the Replay Memory), loading model weights (and the Replay Memory), enabling constant random action, and experimenting with different reward structures (where 'score' = reward based on how much the most recent action increased the game's score, 'score_bug' = reward based on the current score (makes every action seem better than the last), 'sum' = reward based on the sum of the tiles on the board (doesn't directly incentivize merging to get greater numbers, but may incentivize compression/merging to fit as many tiles as possible onto the board), 'score_sum' = (α * score) + ((1 - α) * sum) as the reward, 'merge' = reward whenever a tile that is equal to or greater than 8 comes about as the result of a merging, with greater tiles being rewarded more heavily (this reward structure should result in proper state action values, as states with high tiles next to each other will be valued more highly than states with the same high tiles positioned elsewhere)). 

The 'bad_end' parameter punishes the model for losing (as opposed to getting 0 reward), and the avoid_wall parameter punishes the model for making a move that does not result in the movement of any tiles ("wall smashing"). In my experience, a freshly instantiated model will automatically wall smash, and this behavior takes about 4000 episodes to mostly train out of the model. I advise setting the 'random_duration' parameter to be extremely high (this controls how many episodes the model makes totally random decisions for), otherwise the first few hundred to thousand episodes may take especially long as the model will only have an epsilon chance of not wall smashing.
